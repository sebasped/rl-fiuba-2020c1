{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3zKfVMdTD_Rn",
    "outputId": "95ea3aec-19c2-4865-b4a4-87624bdd400e"
   },
   "outputs": [],
   "source": [
    "# en caso de correrlo en google colab\n",
    "# de esta manera podremos tener la carpeta lib (donde se encuentra en ambiente Gridworld)\n",
    "\n",
    "# import sys\n",
    "#if \"../\" not in sys.path:\n",
    "#  sys.path.append(\"../\") \n",
    "\n",
    "# !git clone https://github.com/julianfm7/cursoRL-FIUBA\n",
    "\n",
    "# necesario en google colab para que sys.path busque\n",
    "# y encuentre la carpeta lib donde se encuentra el ambiente Gridworld\n",
    "\n",
    "# !mv cursoRL-FIUBA cursoRLFIUBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Io7yXe1_QlsM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amUc7zdoCZp0"
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jdQ3cGdCZqS"
   },
   "source": [
    "Grid World es el ambiente del libro de Sutton del capítulo 4. Un agente está en una grilla de MxN y el objetivo es llegar al estado terminal esquina superior izquierda o esquina inferior derecha.\n",
    "\n",
    "Por ejemplo, una grilla de 4x4 se ve así:\n",
    "\n",
    "T  o  o  o <br>\n",
    "o  o  o  o <br>\n",
    "o  x  o  o <br>\n",
    "o  o  o  T\n",
    "\n",
    "x es la posición del agente. T son los estados terminales.\n",
    "\n",
    "El agente puede ir hacia arriba(0), la derecha(1), abajo(2), izquierda(3). Si se choca con las paredes se queda estático. Cada movimiento 'cuesta' una unidad de reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6FB67dVCZqU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  x\n",
      "o  o  o  T\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env._render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26rmh9F9CZqi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  x\n"
     ]
    }
   ],
   "source": [
    "env.step(2)\n",
    "env._render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WarbsGGsCZqu"
   },
   "source": [
    "El objetivo de este ejercicio es evaluar la política aleatoria (que se mueve en las cuatro direcciones con la misma probabilidad).\n",
    "\n",
    "Recordar las ecuaciones y el algoritmo (de Sutton capítulo 4):\n",
    "\n",
    "<img src=\"https://github.com/julianfm7/cursoRL-FIUBA/blob/master/clase3-RL/Python/ecuacion%204.5.PNG?raw=1\">\n",
    "<img src=\"https://github.com/julianfm7/cursoRL-FIUBA/blob/master/clase3-RL/Python/algoritmo%20de%20evaluacion.PNG?raw=1\">\n",
    "<img src=\"https://github.com/julianfm7/cursoRL-FIUBA/blob/master/clase3-RL/Python/evaluacion.png?raw=1\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n5nJ9sOCZqw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluar una política dado un ambiente y una descripción completa\n",
    "    de la dinámica del ambiente.\n",
    "    https://github.com/julianfm7/cursoRL-FIUBA/blob/master/clase3-RL/Python/evaluacion.png?raw=1\n",
    "    Argumentos:\n",
    "        política: matriz de tamaño [S, A] representando la política.\n",
    "        env: ambiente de OpenAI representando las probabilidades de transición\n",
    "        del ambiente. \n",
    "        env.P[s][a] es una lista de tuplas (probabilidad, próximo_estado, recompensa, done)\n",
    "        env.nS es el número de estados en el ambiente\n",
    "        env.nA es el número de acciones en el ambiente\n",
    "        theta: para la evaluación de la política una vez que la función de valor cambia menos que\n",
    "        theta para todos los estados\n",
    "        discount_factor: factor de descuento gama.\n",
    "        \n",
    "    Retorna:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    # Empezar con función de valor nula\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # TODO: Implementar!\n",
    "        delta = 0\n",
    "        for estado in range(env.nS):\n",
    "            v=0\n",
    "#             for accion in env.P[estado]:\n",
    "#                 for probabilidad, proximo_estado, recompensa, _ in env.P[estado][accion]:\n",
    "#                     v += policy[estado,accion]*probabilidad*(recompensa + discount_factor*V[proximo_estado])\n",
    "            for accion,ac_prob in enumerate(policy[estado]):\n",
    "                for probabilidad, proximo_estado, recompensa, _ in env.P[estado][accion]:\n",
    "                    v += ac_prob*probabilidad*(recompensa + discount_factor*V[proximo_estado])\n",
    "\n",
    "            delta = max(delta,abs(v-V[estado]))\n",
    "            V[estado] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-6yVOBGCZq-"
   },
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcL37GOZCZrK"
   },
   "outputs": [],
   "source": [
    "# Verificar que la evaluación de la política funcionó como esperábamos\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAy-1B5CCZrU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Evaluacion de una Politica.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
